{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-12T05:56:30.220260Z","iopub.execute_input":"2023-11-12T05:56:30.220655Z","iopub.status.idle":"2023-11-12T05:56:30.664465Z","shell.execute_reply.started":"2023-11-12T05:56:30.220625Z","shell.execute_reply":"2023-11-12T05:56:30.663060Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/malayalam-news/HackFake Database for the Hackathon - Sheet1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport matplotlib.pyplot as plt\ntf.get_logger().setLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:56:30.666702Z","iopub.execute_input":"2023-11-12T05:56:30.668922Z","iopub.status.idle":"2023-11-12T05:56:47.412289Z","shell.execute_reply.started":"2023-11-12T05:56:30.668886Z","shell.execute_reply":"2023-11-12T05:56:47.410791Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Read dataset\nFILEPATH='/kaggle/input/malayalam-news'\ndf_train=pd.read_csv(os.path.join(FILEPATH,'HackFake Database for the Hackathon - Sheet1.csv'))[:500]\ndf_train = df_train.drop(['Website (source)', 'Timeline/Date','Title'], axis=1)[2:].reset_index(drop=True)\ndf_train=df_train.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:56:47.415314Z","iopub.execute_input":"2023-11-12T05:56:47.416397Z","iopub.status.idle":"2023-11-12T05:56:47.741907Z","shell.execute_reply.started":"2023-11-12T05:56:47.416341Z","shell.execute_reply":"2023-11-12T05:56:47.740911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:56:47.743267Z","iopub.execute_input":"2023-11-12T05:56:47.744568Z","iopub.status.idle":"2023-11-12T05:56:47.753636Z","shell.execute_reply.started":"2023-11-12T05:56:47.744521Z","shell.execute_reply":"2023-11-12T05:56:47.752382Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['Heading of News Article', 'Text of News Article', 'Hate speech',\n       'Misleading', 'Disinformation', 'Rumor/Hoax', 'Sensationalism',\n       'Credible'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"X=df_train[['Text of News Article']]\n#'Heading of News Article'\ny=df_train[['Hate speech','Misleading','Disinformation','Rumor/Hoax','Sensationalism']]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\ny_train=np.array(y_train).astype('int')\ny_test=np.array(y_test).astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:56:47.756538Z","iopub.execute_input":"2023-11-12T05:56:47.756995Z","iopub.status.idle":"2023-11-12T05:56:48.458737Z","shell.execute_reply.started":"2023-11-12T05:56:47.756929Z","shell.execute_reply":"2023-11-12T05:56:48.457221Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers,layers,Input,Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TextVectorization,Conv1D, MaxPooling1D, LSTM, Bidirectional, Dense, BatchNormalization, Dropout\n\n#Custom CallBack and Scheduler\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    patience=15,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n  0.001,\n  decay_steps=X.shape[0]*20,\n  decay_rate=1,\n  staircase=False)\n\n\ndef tf_model():\n    # Input layer\n    input_layer = Input(shape=(), dtype=tf.string, name='text_input')\n\n    # Preprocessor layer\n    preprocessor_layer = hub.KerasLayer(\n        \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\"\n    )\n    preprocessed_text = preprocessor_layer(input_layer)\n\n    # Encoder layer\n    encoder_layer = hub.KerasLayer(\n        \"https://tfhub.dev/google/LaBSE/2\",\n        trainable=False\n    )\n\n    embedded_text = encoder_layer(preprocessed_text)[\"sequence_output\"]\n\n    # Pass the embeddings through LSTM layer\n    lstm_layer = LSTM(units=64, return_sequences=True)(embedded_text)\n\n    # Bidirectional layer\n    bi_dir_layer = Bidirectional(LSTM(64, return_sequences=False))(lstm_layer)\n\n    # Batch normalization\n    norm_layer = BatchNormalization()(bi_dir_layer)\n\n    # Dense layer\n    den_layer = Dense(units=32, activation='relu')(norm_layer)\n\n    # Dropout layer\n    drop_layer = Dropout(0.2)(den_layer)\n\n    # Dense layer\n    penum_layer = Dense(units=16, activation='relu')(drop_layer)\n\n    # Output layer\n    output_layer = Dense(units=5, activation='sigmoid', name='output')(penum_layer)\n\n    # Create the model\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n    return model\n\n# Create the model\nmodel = tf_model()\n\n# Display the model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:56:48.460852Z","iopub.execute_input":"2023-11-12T05:56:48.461627Z","iopub.status.idle":"2023-11-12T05:58:05.812258Z","shell.execute_reply.started":"2023-11-12T05:56:48.461585Z","shell.execute_reply":"2023-11-12T05:58:05.810822Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n text_input (InputLayer)     [(None,)]                    0         []                            \n                                                                                                  \n keras_layer (KerasLayer)    {'input_mask': (None, 128)   0         ['text_input[0][0]']          \n                             , 'input_type_ids': (None,                                           \n                              128),                                                               \n                              'input_word_ids': (None,                                            \n                             128)}                                                                \n                                                                                                  \n keras_layer_1 (KerasLayer)  {'default': (None, 768),     4709268   ['keras_layer[0][0]',         \n                              'encoder_outputs': [(None   49         'keras_layer[0][1]',         \n                             , 128, 768),                            'keras_layer[0][2]']         \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768),                                                   \n                              (None, 128, 768)],                                                  \n                              'sequence_output': (None,                                           \n                              128, 768),                                                          \n                              'pooled_output': (None, 7                                           \n                             68)}                                                                 \n                                                                                                  \n lstm (LSTM)                 (None, 128, 64)              213248    ['keras_layer_1[0][14]']      \n                                                                                                  \n bidirectional (Bidirection  (None, 128)                  66048     ['lstm[0][0]']                \n al)                                                                                              \n                                                                                                  \n batch_normalization (Batch  (None, 128)                  512       ['bidirectional[0][0]']       \n Normalization)                                                                                   \n                                                                                                  \n dense (Dense)               (None, 32)                   4128      ['batch_normalization[0][0]'] \n                                                                                                  \n dropout (Dropout)           (None, 32)                   0         ['dense[0][0]']               \n                                                                                                  \n dense_1 (Dense)             (None, 16)                   528       ['dropout[0][0]']             \n                                                                                                  \n output (Dense)              (None, 5)                    85        ['dense_1[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 471211398 (1.76 GB)\nTrainable params: 284293 (1.08 MB)\nNon-trainable params: 470927105 (1.75 GB)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss='binary_crossentropy',\n        metrics=['acc']\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:58:05.813999Z","iopub.execute_input":"2023-11-12T05:58:05.814788Z","iopub.status.idle":"2023-11-12T05:58:05.855311Z","shell.execute_reply.started":"2023-11-12T05:58:05.814749Z","shell.execute_reply":"2023-11-12T05:58:05.853779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,epochs=10,callbacks=[early_stopping],validation_data=[X_test,y_test],batch_size=200)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:58:05.857588Z","iopub.execute_input":"2023-11-12T05:58:05.858061Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n2/2 [==============================] - 162s 89s/step - loss: 0.7308 - acc: 0.2711 - val_loss: 0.6936 - val_acc: 0.1939\nEpoch 2/10\n2/2 [==============================] - 155s 85s/step - loss: 0.6630 - acc: 0.2922 - val_loss: 0.6904 - val_acc: 0.3091\nEpoch 3/10\n2/2 [==============================] - ETA: 0s - loss: 0.6317 - acc: 0.3373 ","output_type":"stream"}]},{"cell_type":"code","source":"model.predict(X_train.iloc[5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(X_test.iloc[10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict([''])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(model, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}